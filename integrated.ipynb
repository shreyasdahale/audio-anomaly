{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from sklearn import preprocessing \n",
    "import winsound\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data and conversion to spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = []\n",
    "maneuvering_direction = []\n",
    "fault = []\n",
    "ID = []\n",
    "\n",
    "for file in os.listdir('./Dataset-ArIES/drone_A/A/train/mic1'):\n",
    "    labels = file.split('_')\n",
    "    model_type.append(labels[0])\n",
    "    maneuvering_direction.append(labels[1])\n",
    "    fault.append(labels[2])\n",
    "    ID.append(file)\n",
    "for file in os.listdir('./Dataset-ArIES/drone_B/B/train/mic1'):\n",
    "    labels = file.split('_')\n",
    "    model_type.append(labels[0])\n",
    "    maneuvering_direction.append(labels[1])\n",
    "    fault.append(labels[2])\n",
    "    ID.append(file)\n",
    "for file in os.listdir('./Dataset-ArIES/drone_C/C/train/mic1'):\n",
    "    labels = file.split('_')\n",
    "    model_type.append(labels[0])\n",
    "    maneuvering_direction.append(labels[1])\n",
    "    fault.append(labels[2])\n",
    "    ID.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./Dataset/drone_A/A/train/mic2'):\n",
    "    labels = file.split('_')\n",
    "    model_type.append(labels[0])\n",
    "    maneuvering_direction.append(labels[1])\n",
    "    fault.append(labels[2])\n",
    "    ID.append(file)\n",
    "for file in os.listdir('./Dataset/drone_B/B/train/mic2'):\n",
    "    labels = file.split('_')\n",
    "    model_type.append(labels[0])\n",
    "    maneuvering_direction.append(labels[1])\n",
    "    fault.append(labels[2])\n",
    "    ID.append(file)\n",
    "for file in os.listdir('./Dataset/drone_C/C/train/mic2'):\n",
    "    labels = file.split('_')\n",
    "    model_type.append(labels[0])\n",
    "    maneuvering_direction.append(labels[1])\n",
    "    fault.append(labels[2])\n",
    "    ID.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ID, model_type, maneuvering_direction, fault])\n",
    "df = pd.DataFrame(data.T, columns=['ID', 'model_type', 'maneuvering_direction', 'fault']) \n",
    "df = pd.get_dummies(df, columns=['model_type', 'maneuvering_direction', 'fault'], dtype=int)\n",
    "df.to_csv('train.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic1 = []\n",
    "mic2 = []\n",
    "for file in os.listdir('./Dataset/drone_A/A/train/mic1'):\n",
    "    filename = os.path.join('./Dataset/drone_A/A/train/mic1', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic1.append(audio_data)\n",
    "for file in os.listdir('./Dataset/drone_A/A/train/mic2'):\n",
    "    filename = os.path.join('./Dataset/drone_A/A/train/mic2', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic2.append(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./Dataset/drone_B/B/train/mic1'):\n",
    "    filename = os.path.join('./Dataset/drone_B/B/train/mic1', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic1.append(audio_data)\n",
    "for file in os.listdir('./Dataset/drone_B/B/train/mic2'):\n",
    "    filename = os.path.join('./Dataset/drone_B/B/train/mic2', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic2.append(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./Dataset/drone_C/C/train/mic1'):\n",
    "    filename = os.path.join('./Dataset/drone_C/C/train/mic1', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic1.append(audio_data)\n",
    "for file in os.listdir('./Dataset/drone_C/C/train/mic2'):\n",
    "    filename = os.path.join('./Dataset/drone_C/C/train/mic2', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic2.append(audio_data)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sg(audio_data, sr=22050):\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
    "    # mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, S=spectrogram, n_mfcc=30)\n",
    "    # for i in range(len(spectrogram)):\n",
    "    #     for j in range(len(spectrogram[i])):\n",
    "    #           spectrogram[i][j] = spectrogram[i][j]/np.max(abs(spectrogram[i]))\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, _ = librosa.load('./Dataset/drone_A/A/test/mic1/A_B_MF1_0_ConstructionSite_6_snr=10.926310539796413.wav')\n",
    "librosa.display.specshow(librosa.stft(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = []\n",
    "for i in tqdm(range(len(mic1))):\n",
    "    train_array.append(compute_sg(mic1[i]))\n",
    "for i in tqdm(range(len(mic1))):\n",
    "    train_array.append(compute_sg(mic2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.array(train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_2d = train_array.reshape(len(train_array), train_array.shape[1]*train_array.shape[2])\n",
    "np.savetxt('sg_train.csv', train_array_2d, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array_2d = np.loadtxt('sg_train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = train_array_2d.reshape(len(train_array_2d), 128, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winsound.Beep(400, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tensor = torch.tensor(train_array, dtype=torch.float32)\n",
    "training_tensor = training_tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.read_csv('train.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maneuvering_direction = output_data[[4, 5, 6, 7, 8, 9]].values[1:]\n",
    "maneuvering_direction = maneuvering_direction.astype('int32')\n",
    "fault = output_data[[10, 11, 12, 13, 14, 15, 16, 17,18]].values[1:]\n",
    "fault = fault.astype('int32')\n",
    "model_type = output_data[[1, 2, 3]].values[1:]\n",
    "model_type = model_type.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = torch.tensor(model_type, dtype=torch.float16).cuda()\n",
    "y2 = torch.tensor(maneuvering_direction, dtype=torch.float16).cuda()\n",
    "y3 = torch.tensor(fault, dtype=torch.float16).cuda()\n",
    "Xtr = torch.tensor(training_tensor, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(Xtr, y1, y2, y3)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.manual_seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.softplus1 = nn.Softplus(beta=1, threshold=20)  \n",
    "        self.softplus2 = nn.Softplus(beta=1, threshold=20)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.softplus1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        shortcut = self.shortcut(x)\n",
    "        out += shortcut\n",
    "        out = self.softplus2(out)  \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.sp = nn.Softplus(inplace=True)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.sp(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        return out\n",
    "\n",
    "class mtl(nn.Module):\n",
    "    def __init__(self, num_classes1=3, num_classes2=6, num_classes3=9):\n",
    "        super(mtl, self).__init__()\n",
    "        self.resnet18 = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "        self.fc1 = nn.Linear(512, num_classes1)\n",
    "        self.fc2 = nn.Linear(512, num_classes2)\n",
    "        self.fc3 = nn.Linear(512, num_classes3)\n",
    "        self.sm1 = nn.Softmax(dim=1)#dim=1)\n",
    "        self.sm2 = nn.Softmax(dim=1)#dim=1)\n",
    "        self.sm3 = nn.Softmax(dim=1)#dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.resnet18(x)\n",
    "        out1 = self.sm1(self.fc1(out))#, dim=1)\n",
    "        out2 = self.sm2(self.fc2(out))#, dim=1)\n",
    "        out3 = self.sm3(self.fc3(out))#, dim=1)\n",
    "        return out1, out2, out3\n",
    "\n",
    "model = mtl(num_classes1=3, num_classes2=6, num_classes3=9).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('nnweights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch_idx, (data, target1, target2, target3) in enumerate(dataloader):\n",
    "        data, target1, target2, target3 = data.cuda(), target1.cuda(), target2.cuda(), target3.cuda()\n",
    "        outputs1, outputs2, outputs3 = model(data)\n",
    "\n",
    "        loss1 = criterion(outputs1, target1)\n",
    "        loss2 = criterion(outputs2, target2)\n",
    "        loss3 = criterion(outputs3, target3)\n",
    "        total_loss = loss1 + loss2 + loss3\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        if (batch_idx+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Total Loss: {total_loss.item():.4f}')\n",
    "# winsound.Beep(440, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing (I had to test in batches as it was showing memory error so I have used DataLoader here as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic1_test = []\n",
    "mic2_test = []\n",
    "for file in os.listdir('./Dataset-ArIES/drone_A/A/test/mic1'):\n",
    "    filename = os.path.join('./Dataset-ArIES/drone_A/A/test/mic1', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic1_test.append(audio_data)\n",
    "for file in os.listdir('./Dataset-ArIES/drone_A/A/test/mic2'):\n",
    "    filename = os.path.join('./Dataset-ArIES/drone_A/A/test/mic2', file)\n",
    "    audio_data, sample_r = librosa.load(filename)\n",
    "    mic2_test.append(audio_data)\n",
    "\n",
    "for file in os.listdir('./Dataset-ArIES/drone_B/B/test/mic1'):\n",
    "    filename = os.path.join('./Dataset-ArIES/drone_B/B/test/mic1', file)\n",
    "    y, _ = librosa.load(filename)\n",
    "    mic1_test.append(y)\n",
    "for file in os.listdir('./Dataset-ArIES/drone_B/B/test/mic2'):\n",
    "    filename = os.path.join('./Dataset-ArIES/drone_B/B/test/mic2', file)\n",
    "    y, _ = librosa.load(filename)\n",
    "    mic2_test.append(y)\n",
    "\n",
    "for file in os.listdir('./Dataset-ArIES/drone_C/C/test/mic1'):\n",
    "    filename = os.path.join('./Dataset-ArIES/drone_C/C/test/mic1', file)\n",
    "    y, _ = librosa.load(filename)\n",
    "    mic1_test.append(y)\n",
    "for file in os.listdir('./Dataset-ArIES/drone_C/C/test/mic2'):\n",
    "    filename = os.path.join('./Dataset-ArIES/drone_C/C/test/mic2', file)\n",
    "    y, _ = librosa.load(filename)\n",
    "    mic2_test.append(y)\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mic1_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sg_array_test \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mmic1_test\u001b[49m))):\n\u001b[0;32m      3\u001b[0m     sg_array_test\u001b[38;5;241m.\u001b[39mappend(compute_sg(mic1_test[i]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mic1_test' is not defined"
     ]
    }
   ],
   "source": [
    "sg_array_test = []\n",
    "for i in tqdm(range(len(mic1_test))):\n",
    "    sg_array_test.append(compute_sg(mic1_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(mic2_test))):\n",
    "    sg_array_test.append(compute_sg(mic2_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_array_test = np.array(sg_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_array_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_array_test_2d = sg_array_test.reshape(sg_array_test.shape[0], sg_array_test.shape[1]*sg_array_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sg_test.csv', sg_array_test_2d, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_array_test_2d = np.loadtxt('sg_test.csv', delimiter=',')\n",
    "sg_array_test = sg_array_test_2d.reshape(len(sg_array_test_2d), 128, 22)\n",
    "X_test = torch.tensor(sg_array_test, dtype=torch.float32)\n",
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader1 = DataLoader(X_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_mapping = {0: 'A', 1: 'B', 2: 'C'}\n",
    "maneuvering_direction_mapping = {0: 'B', 1: 'C', 2: 'CC', 3: 'F', 4: 'L', 5: 'R'}\n",
    "fault_mapping = {0: 'MF1', 1: 'MF2', 2: 'MF3', 3: 'MF4', 4: 'N', 5: 'PC1', 6: 'PC2', 7: 'PC3', 8: 'PC4'}\n",
    "\n",
    "model_type_pred = []\n",
    "maneuvering_direction_pred = []\n",
    "fault_pred = []\n",
    "\n",
    "with torch.no_grad(), tqdm(total=len(dataloader1)) as progress_bar:\n",
    "    for batch_idx, (data) in enumerate(dataloader1):\n",
    "        output1, output2, output3 = model(data)\n",
    "        \n",
    "        m1_indices = output1.argmax(dim=1).cpu().numpy()\n",
    "        m2_indices = output2.argmax(dim=1).cpu().numpy()\n",
    "        m3_indices = output3.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        model_type_pred.extend([model_type_mapping.get(idx) for idx in m1_indices])\n",
    "        maneuvering_direction_pred.extend([maneuvering_direction_mapping.get(idx) for idx in m2_indices])\n",
    "        fault_pred.extend([fault_mapping.get(idx) for idx in m3_indices])\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "\n",
    "model_type_pred = np.array(model_type_pred)\n",
    "maneuvering_direction_pred = np.array(maneuvering_direction_pred)\n",
    "fault_pred = np.array(fault_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([model_type_pred, maneuvering_direction_pred, fault_pred])\n",
    "df = pd.DataFrame(data.T, columns=['model_type', 'maneuvering_direction', 'fault']) \n",
    "df.to_csv('predwithoutid.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predwithoutid = pd.read_csv('predwithoutid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predwithoutid.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = []\n",
    "# Extract labels from file paths and store them in a list\n",
    "for file in os.listdir('./Dataset-ArIES/drone_A/A/test/mic1'):\n",
    "    ID.append(file +'_mic1')\n",
    "for file in os.listdir('./Dataset-ArIES/drone_B/B/test/mic1'):\n",
    "    ID.append(file + '_mic1')\n",
    "for file in os.listdir('./Dataset-ArIES/drone_C/C/test/mic1'):\n",
    "    ID.append(file + '_mic1')\n",
    "for file in os.listdir('./Dataset-ArIES/drone_A/A/test/mic2'):\n",
    "    ID.append(file + '_mic2')\n",
    "for file in os.listdir('./Dataset-ArIES/drone_B/B/test/mic2'):\n",
    "    ID.append(file + '_mic2')\n",
    "for file in os.listdir('./Dataset-ArIES/drone_C/C/test/mic2'):\n",
    "    ID.append(file + '_mic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predwithoutid.insert(0, 'ID', ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predwithoutid['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predwithoutid.to_csv('pred1.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
